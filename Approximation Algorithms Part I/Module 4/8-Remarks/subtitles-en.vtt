WEBVTT

1
00:00:00.000 --> 00:00:04.801
[MUSIC]

2
00:00:07.198 --> 00:00:10.805
There are many other things
one could say about set cover.

3
00:00:10.805 --> 00:00:13.170
Let me just end with a few remarks.

4
00:00:15.040 --> 00:00:20.040
The result, we have proof, is that
the sample-and-iterate algorithm gives us

5
00:00:20.040 --> 00:00:24.350
as an output a collection
of sets forming a set cover

6
00:00:24.350 --> 00:00:28.827
with cost that is on average,
at most (ln(n) + 1) times OPT.

7
00:00:30.380 --> 00:00:32.200
Can we do better than this?

8
00:00:32.200 --> 00:00:37.340
Can we do something whose cost is
always at most (ln(n)+1) times OPT?

9
00:00:37.340 --> 00:00:39.890
Can we have a faster algorithm?

10
00:00:39.890 --> 00:00:43.160
The answer to these questions is yes and

11
00:00:43.160 --> 00:00:47.160
yes, with one single
answer to both questions.

12
00:00:48.950 --> 00:00:53.130
Linear programming that we use in
the first part of our algorithm takes

13
00:00:53.130 --> 00:00:58.959
polynomial time, but usually it is slower
than purely combinatorial algorithms.

14
00:01:00.210 --> 00:01:04.980
So let us try to replace linear
programming by a combinatorial algorithm.

15
00:01:06.720 --> 00:01:09.530
The way to do it is to go greedy.

16
00:01:11.250 --> 00:01:12.170
How do we do this?

17
00:01:13.510 --> 00:01:19.400
We do a loop that repeats choosing
sets until we have a set cover.

18
00:01:19.400 --> 00:01:22.830
So it is similar to
the sample-and-iterate algorithm.

19
00:01:24.120 --> 00:01:25.460
And what do we repeat?

20
00:01:25.460 --> 00:01:28.970
At every step we choose a set,
the best possible set.

21
00:01:30.200 --> 00:01:35.400
The set we choose covers a certain
number of elements that are not yet

22
00:01:35.400 --> 00:01:39.780
covered, and
it costs a certain value, C sub S.

23
00:01:39.780 --> 00:01:44.630
Let us pick the best possible set,
the set that maximizes the ratio

24
00:01:44.630 --> 00:01:48.700
of the number of new elements covered,
over the cost of the set.

25
00:01:50.770 --> 00:01:53.945
When you do this,
that is a natural greedy algorithm.

26
00:01:53.945 --> 00:01:59.845
And what is known about this algorithm
is that it also outputs a set cover,

27
00:01:59.845 --> 00:02:05.480
of course, and its cost is always
at most (ln(n) + 1) times OPT.

28
00:02:05.480 --> 00:02:10.406
So it's just as good as the
sample-and-iterate algorithm that we have

29
00:02:10.406 --> 00:02:12.090
studied.

30
00:02:12.090 --> 00:02:16.142
The advantages are first,
greedy is combinatorial.

31
00:02:16.142 --> 00:02:19.130
It doesn't require
solving a linear program.

32
00:02:19.130 --> 00:02:23.526
Second, the quality of a result
is guaranteed in the worst

33
00:02:23.526 --> 00:02:26.497
case instead of being on average only.

34
00:02:26.497 --> 00:02:28.577
But the bounds are the same, and

35
00:02:28.577 --> 00:02:34.240
it is not quite a coincidence the two
algorithms share some similarity.

36
00:02:34.240 --> 00:02:39.240
And you can actually argue that if
you take the randomized algorithm and

37
00:02:39.240 --> 00:02:43.070
you do some properly
done de-randomization,

38
00:02:43.070 --> 00:02:46.690
then you may generate
the greedy algorithm.

39
00:02:46.690 --> 00:02:51.070
So that is another technique to
generate good approximation algorithms.

40
00:02:52.790 --> 00:02:56.630
Can we do better than this,
better than ln(n)?

41
00:02:56.630 --> 00:02:59.360
No, the answer is no.

42
00:02:59.360 --> 00:03:04.640
It is impossible if P is different
from NP to get, in polynomial time,

43
00:03:04.640 --> 00:03:09.950
a result, a set cover that is
better than ln(n) times OPT,

44
00:03:09.950 --> 00:03:12.770
that is always better
than ln(n) times OPT.

45
00:03:12.770 --> 00:03:15.440
So this algorithm is optimal.

46
00:03:15.440 --> 00:03:18.860
It is the best we can hope for
if P is different from NP.

47
00:03:18.860 --> 00:03:21.480
It is the best approximation for
set cover.

48
00:03:22.830 --> 00:03:27.560
Who has worked on this problem and
proved all these nice results?

49
00:03:27.560 --> 00:03:29.400
Many people have worked on this.

50
00:03:29.400 --> 00:03:30.750
Let me just single out a few.

51
00:03:32.480 --> 00:03:35.869
In the 1970s, Chvátal, who is Czech,

52
00:03:35.869 --> 00:03:39.450
designed and
analyzed the greedy algorithm.

53
00:03:41.820 --> 00:03:46.053
In the 1970s, Lovász,
who is Hungarian, designed and

54
00:03:46.053 --> 00:03:48.380
analyzed the greedy algorithm.

55
00:03:49.650 --> 00:03:54.500
In the 1970s, David Johnson,
who's from the US, designed and

56
00:03:54.500 --> 00:03:55.721
analyzed the greedy algorithm.

57
00:03:56.830 --> 00:03:59.300
Wait, they all did the same thing?

58
00:04:00.550 --> 00:04:04.370
Well, these were all
independent pieces of work.

59
00:04:04.370 --> 00:04:09.190
I guess the time was just ripe for
the design of the first

60
00:04:09.190 --> 00:04:13.300
approximation algorithms and
the first developments of the field.

61
00:04:14.350 --> 00:04:18.760
What about optimality, the NP harness
of getting a better approximation?

62
00:04:18.760 --> 00:04:21.480
This is due to Uri Feige from Israel.

63
00:04:22.910 --> 00:04:28.220
Finally, the presentation I gave you on
the last part of a sample-and-iterate

64
00:04:28.220 --> 00:04:33.287
algorithm is largely inspired from
lecture notes by Neal Young using

65
00:04:33.287 --> 00:04:37.830
Wald's inequalities.

66
00:04:37.830 --> 00:04:40.580
So, what have we learned in this chapter?

67
00:04:42.670 --> 00:04:45.440
We have learned a famous problem,
set cover.

68
00:04:45.440 --> 00:04:50.220
We have learned fundamental algorithms and
properties of this basic problem.

69
00:04:51.690 --> 00:04:54.760
This was our first example
of an important concept,

70
00:04:54.760 --> 00:04:57.710
randomization in the design of algorithms.

71
00:04:57.710 --> 00:05:02.430
For approximation algorithms we used
a basic technique, randomized rounding.

72
00:05:02.430 --> 00:05:05.690
Very useful also in other settings
including the one we will see

73
00:05:05.690 --> 00:05:07.330
in the next chapter.

74
00:05:07.330 --> 00:05:13.090
In terms of analysis, the one tool that we
saw that is most basic, most important,

75
00:05:13.090 --> 00:05:17.790
really needs to be remembered if
nothing else, linearity of expectation.

76
00:05:17.790 --> 00:05:21.685
This is really important for
every analysis of randomized algorithms.

77
00:05:21.685 --> 00:05:28.640
Finally, in terms of the way in which
we do the analysis, this slow and

78
00:05:28.640 --> 00:05:34.300
steady way, laying equations one after the
other and and looking at each step, what

79
00:05:34.300 --> 00:05:39.740
is the next step in an orderly fashion is
a little bit like hiking up a mountain.

80
00:05:39.740 --> 00:05:44.610
You have to go slowly and carefully,
one step after the other, left, right,

81
00:05:44.610 --> 00:05:45.570
left, right.

82
00:05:45.570 --> 00:05:49.060
And then you will get to the top
without getting tired along the way.

83
00:05:50.980 --> 00:05:54.252
In the next chapter,
now that we have completed set cover,

84
00:05:54.252 --> 00:05:57.658
we will look at another application
of linear programming and

85
00:05:57.658 --> 00:06:01.403
an interesting randomized rounding for
the multiway cut problem.